<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# classification / 分類問題

### 22140026 / 情報科学科

### Tomohiro Tani / 谷 知拓 / mail@taniii.com / https://taniii.com

### 2024.4.16

---

> パーセプトロンと ADALINE についてそれぞれ説明しなさい。また違いについて説明しなさい。

パーセプトロン :
生物の脳内のニューロンを模倣した人工のモデルである.
特徴量を入力とし受け取り, それぞれの入力に重みをつけ, その和が, ある閾値 (単位ステップ関数) を超えるかどうかで, 分類問題に対する回答 (「属する」か否か) を出力する.

ADALINE :
パーセプトロンの改良であり, 学習段階において, モデルを評価し, 重みを更新することでモデル改善するのに使用する関数が異なる.
従来のパーセプトロンでは, 単位ステップ関数を用いてある閾値を超えるかどうかで出力された, 分類問題への回答の成否結果を用いて, 重みを更新し, モデルを改善していた.
一方, ADALINE は, 重みを更新しモデルを改善するのに, 活性化関数 (シグモイド関数, ReLU 関数, タンジェント関数など) を用いる. これにより, ニューラルネットワークに非線形性を導入することが可能になり, ニューラルネットワークがより複雑な現象を学習することが可能になる.

> 確率的勾配降下法について説明しなさい。

従来の勾配降下法では, 学習データセット全体を使って損失関数の勾配 (損失関数の導関数) を求め, これが損失関数を最も速く減少させる方向を表すので, この勾配に比例してパラメータ (重みとバイアス) を更新する.
一方, 確率的勾配降下法では, 1 つもしくは少数のサンプルデータのみを使用して勾配を計算し, すぐにパラメータを更新することができる.
ノイズが介入する可能性がある欠点がある一方, 学習データのリアルタイムの変化に強くなり, 計算資源を効率的に利用することが可能になる利点がある.

> 勾配降下法で学習する ADALINE と、確率的勾配降下法で学習する ADALINE について、学習率を 0.01 に固定する。このとき、エポック数を 10 にした場合と 1000 にした場合についてそれぞれ学習を行い、収束速度の違いについて考察しなさい。

エポック数 10 の場合 :
勾配降下法では, 十分収束しておらず, 最終的な分類も正しく行えていない. これは, 最適化の回数が不十分であるからと言える.
一方, 確率的勾配降下法では, 十分収束している. 最終的な分類も正しく行えている. これは, 同じエポック数 10 であっても, (エポック数 \* データセットの数) 回だけ最適化が行われるため, 最適化の回数が十分であるからだと言える.

エポック数 1000 の場合 :
勾配降下法, 確率的勾配降下法ともに, 十分収束しており, 最終的な分類も正しく行えている.
確率的勾配降下法の方が, 勾配降下法に比べてやや速く収束していることがわかるが, エポック数 1000 の最適化後の決定境界は, 両者が極めて近いものになっていることが読み取れる.
エポック数 1000 の場合は, どちらの場合でも十分回数の最適化が行えていると言える.
