<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# classification / 分類問題

### 22140026 / 情報科学科

### Tomohiro Tani / 谷 知拓 / mail@taniii.com / https://taniii.com

### 2024.4.26

---

> ロジスティック回帰と ADALINE の違いについて説明しなさい。

ロジスティック回帰も ADALINE も線形分類問題に利用されるアルゴリズムであるが, 活性化関数が異なる.
ロジスティック回帰は, シグモイド関数を用いるため, 結果をあるクラスに所属する確率として得ることが可能である. よって, 確率的な分類が求められるタスクに適している.
一方, ADALINE は, 恒等関数 (= 線形関数) を利用するため, 線形回帰やパターン認識に適している.

> 平均二乗誤差損失と交差エントロピー損失の違いについて説明しなさい。

平均二乗誤差損失と交差エントロピー損失は, どちらも損失関数である.
損失関数とは, 機械学習のモデルがどの程度予測を間違っているかを数値で表す関数であり, モデルの予測値と実際の値との差を計測し, その差を最小化するようにモデルのパラメータを調整するために使用される.

平均二乗誤差損失関数は, 実際の値と予測値の差の二乗を取ることで, 大きな誤差にはより大きなペナルティを与える損失関数である. これにより, モデルがデータの平均的な振る舞いをよく学習することができるが, 一方で, 外れ値に過敏に反応する欠点がある. 回帰タスクに適している.

交差エントロピー損失は, 予測された確率と実際のラベルの間の不一致を測定し, 正解クラスに対する予測確率が低いほど, また, 誤ったクラスに対する予測確率が高いほど, 損失が大きくなる関数である. 分類タスクに適する.

> scikit-learn を用いたロジスティック回帰実装において、正則化の強さを決めるパラメータ C の値を 10^-5、10^3 に変化させ、決定境界がどのように変わるか確認しなさい。また、なぜそのような結果が得られるのか考察しなさい。

結果 :
10**(-5) のとき, 正則化は強く, 決定境界はシンプルで直線的であり, 決定境界に基づくと, データはほとんど `class 0` と `class 2` に分類され, `class 1` のデータは正しく分類されていない.
10**3 のとき, 正則化は弱く, 決定境界は複雑になり, `class 0`, `class 1`, `class 2` は概ね正しく分類されている.

考察 :
正規化が強い場合, 決定境界がシンプルになるため, 学習不足により過少適合 (underfitting) のリスクが大きくなる.
一方, 正規化が弱い場合, 決定境界が複雑になるため, 過学習により過剰学習 (overfitting) のリスクが大きくなる.

---

## notes.

- `ch02` の実行には `scikit-learn` が必要

```bash
pip install -U scikit-learn
```
